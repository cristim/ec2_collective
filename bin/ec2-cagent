#!/usr/bin/env python
# coding: utf-8

from socket import gethostname, setdefaulttimeout
import simplejson as json
import subprocess
import time
import signal
import sys
import yaml
import os
import stat
import getopt
import logging
import pkg_resources
import fcntl

# Required to support threads
from threading import Timer
from threading import Thread
from threading import active_count
from Queue import Queue
from Queue import Empty

# S3 stuff
from boto.s3.key import Key
from boto.s3.connection import S3Connection
from boto.exception import S3ResponseError

# SQS stuff
from boto.sqs.connection import SQSConnection
from boto.sqs.regioninfo import SQSRegionInfo
from boto.sqs import connect_to_region

try:
    pkg_resources.require("Boto>=2.0")
except pkg_resources.VersionConflict, e:
    sys.stderr.write('Boto 2.0 or higher is required - exiting\n')
    sys.exit(1)

# Set a default timeout on all socket requests
setdefaulttimeout(60)

CFILE='/etc/ec2_collective/ec2-cagent.json'

# Create queues
task_queue = Queue()
done_queue = Queue()
yaml_queue = Queue()
oldmsgs_queue = Queue()

def terminate_process(signum, frame):
    logging.info ('Process asked to exit (SIGTERM) - exiting')
    sys.exit(0)

signal.signal(signal.SIGTERM, terminate_process)

def usage():
    sys.stderr.write('Usage:')
    sys.stderr.write('\t' + sys.argv[0] + '\n\n -f, --foreground\trun script in foreground\n -h, --help\tthis help\n -l, --logfile\tlogfile path ( /var/log/ec2_collective/ec2-cagent.log )\n -p, --pidfile\tpath to pidfile (/var/run/ec2-cagent.pid)\n')
    sys.exit(1)

def set_logging(logfile, foreground):

    log = logging.getLogger()
    logging.getLogger('boto').setLevel(logging.CRITICAL)
    fmt = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")

    if foreground is True:
        ch  = logging.StreamHandler(sys.stdout)
        ch.setFormatter(fmt)
        log.addHandler(ch)
    else:
        fh = logging.handlers.WatchedFileHandler(logfile)
        fh.setFormatter(fmt)
        log.addHandler(fh)

    if CFG['general']['log_level'] not in ['INFO', 'WARN', 'ERROR', 'DEBUG', 'CRITICAL' ]:
        sys.stderr.write('Log level: ' + CFG['general']['log_level'] + ' is invalid\n')
        sys.exit(1)

    if CFG['general']['log_level'] == 'INFO':
        logging.getLogger().setLevel(logging.INFO)
    elif CFG['general']['log_level'] == 'WARN':
        logging.getLogger().setLevel(logging.WARNING)
    elif CFG['general']['log_level'] == 'ERROR':
        logging.getLogger().setLevel(logging.ERROR)
    elif CFG['general']['log_level'] == 'DEBUG':
        logging.getLogger().setLevel(logging.DEBUG)
    elif CFG['general']['log_level'] == 'CRITICAL':
        logging.getLogger().setLevel(logging.CRITICAL)

def acquire_lock(pidfile):

    try:
        f = open(pidfile, 'a')
    except IOError, err:
        sys.stderr.write('Unable to open lockfile: ' + str(pidfile) + ' (' + str(err) + ')\n')
        sys.exit(1)

    try:
        fcntl.flock(f, fcntl.LOCK_EX|fcntl.LOCK_NB)
        return f
    except IOError, err:
        sys.stderr.write('Unable to acquire lockfile: ' + str(pidfile) + ' (' + str(err) + ')\n')
        sys.exit(1)

def initialize(logfile='/var/log/ec2_collective/ec2-cagent.log', pidfile='/var/run/ec2-cagent.pid', foreground=False):

    try:
        opts, args = getopt.getopt(sys.argv[1:], 'hfl:p:', ['foreground', 'help', 'logfile', 'pidfile'])

    except getopt.GetoptError, err:
        sys.stderr.write('Failed to parse arguments: (%s)\n' % (err))
        sys.exit(1)

    for o, a in opts:
        if o in ('-h', '--help'):
            usage()
        elif o in ('-f', '--foreground'):
            foreground=True
        elif o in ('-l', '--logfile'):
            logfile=a
        elif o in ('-p', '--pidfile'):
            pidfile=a

    get_config()
    set_logging(logfile,foreground)

    stdoutlog = os.path.dirname(logfile) + '/ec2-cagent.stdout'
    stderrlog = os.path.dirname(logfile) + '/ec2-cagent.stderr'

    return (foreground, logfile, pidfile, stdoutlog, stderrlog)

def write_yaml_to_queue ( message, sqs_facts_queue ):

    # Make sure the facts message contains a hostname
    message.update({ 'ec2_cagent_hostname' : gethostname() })

    logging.debug('write_yaml_to_queue: Writing facts to sqs facts queue')
    write_sqs_msg (message, sqs_facts_queue)

def get_yaml_facts ():
    logging.debug('get_yaml_facts: Loading facts')
    sqs_facts_queue = establish_sqs_conn(CFG['aws']['sqs_facts_queue'])

    # Define dict for storage
    dataMap = {}
    valid_yaml_files = []

    yaml_files = CFG['general']['yaml_facts_path'].split(',')
    for yaml_file in yaml_files:

        if not os.path.exists(yaml_file):
            logging.error( yaml_file + ' file does not exist')
            continue

        # If we get to here - the file looks good
        valid_yaml_files.append(yaml_file)

    for yaml_file in valid_yaml_files:
        logging.debug('get_yaml_facts: Will load facts from ' + str(yaml_file))

        f = open(yaml_file, 'r')
        try:
            data_from_file = yaml.safe_load(f)
        except:
            logging.error('Error while loading yaml from file (format problems) ' + str(yaml_file))
            continue

        if type(data_from_file) is str:
            logging.debug('get_yaml_facts: Facts file looks to be string ' + str(yaml_file))
            fact_string = data_from_file.split()

            for fact in fact_string:
                newfact = {fact:'None'}
                dataMap.update(newfact)
        else:
            logging.debug('get_yaml_facts: We assume facts file is in yaml format ' + str(yaml_file))
            dataMap.update(data_from_file)
            f.close()

    if len(dataMap) > 0:
        logging.info('Facts loaded from file(s)')
        logging.debug('get_yaml_facts:' + str(dataMap))
        yaml_queue.put(dataMap)

        if CFG['general']['use_facts_queue'] == True:
            write_yaml_to_queue(dataMap, sqs_facts_queue)
    else:
        logging.debug('get_yaml_facts: Facts are empty')

def cli_mode (message):

    try:
        logging.debug ('cli_mode: Executing: ' + str(message['cmd']))
        o = subprocess.Popen(message['cmd'], shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, close_fds=True)
        output = o.communicate()[0]
        rc = o.poll()

    except OSError, e:
        output = ('Failed to execute ' + message['cmd'] + ' (%d) %s \n' % (e.errno, e.strerror))
        rc = e.errno

    response={'mode': message['mode'], 'output': output, 'rc': rc, 'ts':time.time(), 'msg_id':message['orgid'], 'hostname':gethostname()};
    return response

def write_to_file (payload, identifier):
    script_file = '/tmp/' + 'ec2-cagent-' + identifier

    try:
        f = open (script_file, 'w')
        f.write (payload)
        f.close()
    except IOError, e:
        logging.error ('Failed to write payload to ' + script_file + ' (%d) %s \n' % (e.errno, e.strerror))
        return (False, 'Failed to write payload to file ' + script_file)

    os.chmod(script_file, stat.S_IRWXU)

    return (True, script_file)

def fetch_s3_file ( s3_path, identifier ):

    bucket_name=os.path.dirname(s3_path)
    file_name=os.path.basename(s3_path)

    logging.debug ('fetch_s3_file: Fetching script ' + file_name + ' from s3 bucket ' + bucket_name )

    try:
        s3conn =S3Connection()
    except S3ResponseError:
        logging.error ('Failed to connect to S3')
        return (False, 'Failed to connect to S3')

    try:
        bucket = s3conn.get_bucket(bucket_name)
    except S3ResponseError:
        logging.error ('Failed to get bucket ' + bucket_name )
        return (False, 'Failed to get bucket ' + bucket_name )

    k = Key(bucket)
    k.key = file_name

    try:
        payload = k.get_contents_as_string()
    except S3ResponseError, e:
        logging.error ('Failed to get file from bucket ' + str(e))
        return (False, 'Failed to get file from bucket')

    return write_to_file(payload, identifier )

def script_mode (message):

    # Put script on filesystem
    if message['mode'] == 's3':
        (file_written, script_file ) = fetch_s3_file(message['cmd'], message['batch_msg_id'])
    else:
        (file_written, script_file ) = write_to_file(message['payload'], message['batch_msg_id'])

    if file_written is False:
        # script_file will include error message
        response={'mode': message['mode'], 'output': script_file, 'rc': '255', 'ts':time.time(), 'msg_id':message['orgid'], 'hostname':gethostname()};
        return response

    command=script_file

    # If we are carrying script parameters, please use them
    if message['script_param'] is not None:
        command = str(command) + ' ' + str(message['script_param'])

    try:
        logging.debug ('script_mode: Executing script: ' + str(message['cmd']) + ' with params: ' + str(message['script_param']))
        o = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, close_fds=True)
        output = o.communicate()[0]
        rc = o.poll()

    except OSError, e:
        logging.error ('Failed to execute ' + message['cmd'] )
        output = ('Failed to execute ' + message['cmd'] + ' (%d) %s \n' % (e.errno, e.strerror))
        rc = e.errno

    try:
        logging.debug ('script_mode: Deleting file with script '  + script_file)
        os.remove(script_file)
    except OSError, e:
        logging.error ('Failed to delete script file ' + script_file + ' (%d) %s \n' % (e.errno, e.strerror))

    response={'mode': message['mode'], 'output': output, 'rc': rc, 'ts':time.time(), 'msg_id':message['orgid'], 'hostname':gethostname()};
    return response

def clean_old_msgs(read_msgs):

    # Clear references to old messages ( keep messages until we haven't done anything for 2 minutes )
    # AWS SQS queues must have this setting
    # Message Retention Period:     1 minutes
    # That will allow AWS to purge old messages a minute before we might pick it up again.

    for id, lastseen in read_msgs.items():
        if lastseen < (time.time() - 120 ):
            logging.debug('clean_old_msgs: removing old message')
            del read_msgs[id]

    return read_msgs

def receive_sqs_msg ( yaml_facts ):
    sqs_read_queue = establish_sqs_conn(CFG['aws']['sqs_read_queue'])

    try:
        read_msgs = oldmsgs_queue.get(False)
        read_msgs = clean_old_msgs(read_msgs)
        logging.debug('receive_sqs_msg: Old received messages re-read')
    except Empty:
            logging.debug('receive_sqs_msg: oldmsgs queue is empty')
            read_msgs = {}

    logging.debug('receive_sqs_msg: Looking for new messages on SQS')

    logging.info ('Attempting to get messages from SQS')
    try:
        msgs = sqs_read_queue.get_messages(num_messages=10, visibility_timeout=0, wait_time_seconds=20)
    except:
        logging.error('Receive sqs messages thread exiting...')
        sys.exit(0)

    logging.info ('Request messages from SQS success')

    for msg in msgs:
        if msg.id in read_msgs:
            continue
        else:
            read_msgs[msg.id] = time.time()

        message=json.loads(msg.get_body())
        batch_msg_id = message['batch_msg_id']
        wf = message['wf']
        wof = message['wof']
        message['orgid'] = msg.id

        # We send multiple duplicate messages - lets avoid handling all of them
        if batch_msg_id in read_msgs:
            continue
        else:
            read_msgs[batch_msg_id] = time.time()

        if fact_lookup(wf, wof, yaml_facts ):
            logging.debug('receive_sqs_msg: SQS messages did not pass filter')
            read_msgs[msg.id] = time.time()
            continue

        logging.debug('receive_sqs_msg: New valid SQS message received')

        logging.debug('receive_sqs_msg: Putting SQS message on task queue')
        task_queue.put(message)

    # End by putting all old messages on the queue
    logging.debug('receive_sqs_msg: Putting received messages on oldmsgs queue')
    oldmsgs_queue.put(read_msgs)

def process_msg ( message ):

    cmd_str = str(message['cmd'])
    mode = str(message['mode'])
    ts = str(message['ts'])

    if mode in ['ping', 'count']:
        logging.info('Performing ' + str(mode) + ' reply')
        response={'mode': mode, 'output': ts, 'rc': '0', 'ts':time.time(), 'msg_id':message['orgid'], 'hostname':gethostname()};
        return response
    else:
        logging.info("Performing ping reply - for discovery purpose")
        response={'mode': 'ping', 'output': ts, 'rc': '0', 'ts':time.time(), 'msg_id':message['orgid'], 'hostname':gethostname(), 'discovery':1};
        done_queue.put(response)

    if mode == 'cli':
        logging.info("Performing command execution")
        response = cli_mode(message)
    elif mode in [ 'script', 's3' ]:
        logging.info('Performing ' + mode + ' execution')
        response = script_mode (message)
    else:
        logging.error('Unknown mode: ' + mode)
        response =  'Unknown command ' + cmd_str
        response={'mode': mode, 'output': response, 'rc': '0', 'ts':time.time(), 'msg_id':message['orgid'], 'hostname':gethostname()};

    return response

def receive_queue_msg ( ):

    try:
        message = task_queue.get(False)
        logging.debug('receive_queue_msg: Task read from task queue')
        done_queue.put(process_msg(message))
    except Empty:
            logging.debug('receive_queue_msg: Task queue is empty')
    except:
            logging.debug('receive_queue_msg: Unknown error in processing task queue')

    logging.debug('receive_queue_msg: Worker ended - terminating')

def write_sqs_msg (response, sqs_write_queue):

    # response - dict
    # response_in_json - json string
    # message - sqs message object

    try:
        response_in_json=json.dumps(response)
        logging.debug ('write_sqs_msg: Response size in json ' + str(len(response_in_json)))
        message = sqs_write_queue.new_message(response_in_json)
    except:
        logging.error('Failed to create message object - check queue name')
        return False

    if len(message) >= 65536 and 'output' in response:
        logging.warn ('Response message is too big to put on SQS ' + str(len(message)) + ' - sending last 1000 characters')

        # Let's make sure we can actually decrease response....
        if len(response['output']) > 1000:
            response['output'] = '--- OUTPUT CAPPED ---\n' + str(response['output'][-1000:])
        else:
            logging.error ('Message size is too big to put on sqs, but it is not in the actual output!')
            response['output'] = 'Panic response from agent - please check exit code'

        try:
            response_in_json=json.dumps(response)
            logging.debug ('write_sqs_msg: Response size in json ' + str(len(response_in_json)))
            message = sqs_write_queue.new_message(response_in_json)
        except:
            logging.error('Failed to create message object - check queue name')
            return False

    elif len(message) >= 65536:
        logging.error ('Response message is too big to put on SQS ' + str(len(message)))
        return False

    # Retry 3 times
    for i in range(0, 3):
        logging.info ('Attempting write to SQS')
        try:
            org = sqs_write_queue.write(message)
        except:
            logging.error('Failed to write to sqs - quitting thread')
            sys.exit(0)
        if org.id is None:
            logging.error ('Failed to write response message to SQS')
            time.sleep(1)
        else:
            logging.info ('Write to SQS success')
            logging.debug('write_sqs_msg: Wrote response to SQS')
            return True

    return False

def get_config():
    # CFILE
    if not os.path.exists(CFILE):
        sys.stderr.write('Config file (%s) does not exist\n' % (CFILE) )
        sys.exit(1)

    try:
        f = open(CFILE, 'r')
    except IOError, e:
        sys.stderr.write('Failed to open config file (%s)\n' % (CFILE) )
        sys.exit(1)

    try:
        global CFG
        CFG=json.load(f)
    except (TypeError, ValueError), e:
        sys.stderr.write('Error in configuration file (%s)\n' % (CFILE) )
        sys.exit(1)

def write_pidfile(pidfile):

    try:
        f = open (pidfile, 'w')
        f.write (str(os.getpid()))
    except IOError, e:
        sys.stderr.write("Failed to write pid to pidfile (%s): (%d) %s\n" % (pidfile, e.errno, e.strerror))
        sys.exit(1)

def daemonize (foreground, pidfile, stdin='/dev/null', stdout='/dev/null', stderr='/dev/null' ):
    lockfile=acquire_lock(pidfile)

    if foreground is True:
        logging.info ('Running in the foreground')
        write_pidfile(pidfile)
        return lockfile

    try:
        pid = os.fork( )
        if pid > 0:
            sys.exit(0) # Exit first parent.
    except OSError, e:
        sys.stderr.write("fork #1 failed: (%d) %s\n" % (e.errno, e.strerror))
        sys.exit(1)
    # Decouple from parent environment.
    os.chdir("/")
    os.umask(022)
    os.setsid( )
    # Perform second fork.
    try:
        pid = os.fork( )
        if pid > 0:
            sys.exit(0) # Exit second parent.
    except OSError, e:
        sys.stderr.write("fork #2 failed: (%d) %s\n" % (e.errno, e.strerror))
        sys.exit(1)
    # The process is now daemonized, redirect standard file descriptors.
    for f in sys.stdout, sys.stderr: f.flush( )
    si = file(stdin, 'r')
    so = file(stdout, 'a+')
    se = file(stderr, 'a+', 0)
    os.dup2(si.fileno( ), sys.stdin.fileno( ))
    os.dup2(so.fileno( ), sys.stdout.fileno( ))
    os.dup2(se.fileno( ), sys.stderr.fileno( ))

    write_pidfile(pidfile)

    sys.stdout.write('Daemon started with pid %d\n' % os.getpid( ) )

    sys.stderr.flush()
    sys.stdout.flush()

    return lockfile

def fact_lookup (wf, wof, yaml_facts ):

    # True - Skip
    # False - Process

    # WOF
    # Return True if we have the fact ( just on should skip message )
    # Return False if we don't have the fact

    # WF
    # Return False if we have all the fact ( all facts must match )
    # Return True if we don't have the fact

   # Execptions where we need to be careful

    # If yaml facts is false
    if CFG['general']['yaml_facts'] == False:
        if wf is None and wof is None:
            logging.debug('fact_lookup: Process message - yaml_facts is False and message contains no facts')
            return False
        else:
            logging.debug('fact_lookup: Skip message - yaml_facts is False but message contains facts')
            return True

    # If yaml facts is true
    if CFG['general']['yaml_facts'] == True:
        # but we have no facts skip everything
        if yaml_facts is None:
            logging.error('Skip message - yaml_facts is True but there are no facts available!')
            return True

        # If nothing is set we process message
        if wf is None and wof is None:
            logging.debug('fact_lookup: Process message - yaml_facts is True but message contains no facts')
            return False

    # If wof is in facts we return True ( skip message )
    if wof is not None:
        wof = wof.split(',')
        for f in wof:
            if '=' in f:
                f = f.split('=')

                if (f[0] in yaml_facts) and (yaml_facts[f[0]] == f[1]):
                    logging.debug('fact_lookup: Skip message - wof matched')
                    return True
            else:
                if f in yaml_facts:
                    logging.debug('fact_lookup: Skip message - wof matched')
                    return True

    # Without is set but we did not find it, if wf is not set we return False ( process message )
    if wf is None:
        logging.debug('fact_lookup: Process message - no wof match and no wf')
        return False

    # If all wf is in facts we return False ( process message )
    no_match=0
    wf = wf.split(',')
    for f in wf:
        if '=' in f:
            f = f.split('=')

            if (f[0] not in yaml_facts) or (yaml_facts[f[0]] != f[1]):
                no_match += 1
        else:
            if f not in yaml_facts:
                no_match += 1

    if no_match == 0 :
        # All facts was found ( process message )
        logging.debug('fact_lookup: Process message - wf matched')
        return False
    else:
        # Facts set was not found - return True ( skip message )
        logging.debug('fact_lookup: Skip message - no wf match')
        return True

def thread_worker():

    thread = Thread(target = receive_queue_msg, args = ())
    thread.daemon=True
    thread.start()

def check_task_queue():
    logging.debug('check_task_queue: Checking task queue')
    if task_queue.qsize() > 0:
        thread_worker()

def check_done_queue():
    logging.debug('check_done_queue: Checking done queue')

    if done_queue.qsize() > 0:
        try:
            response = done_queue.get(False)
            logging.debug('check_done_queue: Response read from done queue')
            sqs_write_queue = establish_sqs_conn(CFG['aws']['sqs_write_queue'])
            write_sqs_msg (response, sqs_write_queue)

        except Empty:
            logging.debug('check_done_queue: Done queue is empty')
        except:
            logging.debug('check_done_queue: Unknown error in processing done queue')

def establish_sqs_conn (queue):

    # Connect with key, secret and region
    try:
        logging.info('Connecting to region ' + str(CFG['aws']['region']))
        conn = connect_to_region(CFG['aws']['region'])
    except:
        logging.error('Could not connect to SQS - check your authentication')
        sys.exit(1)

    try:
        logging.info('Getting queue url for ' + str(queue))
        queue = conn.get_queue(queue)
    except:
        logging.error('Could not determine SQS queue url - check your authentication')
        sys.exit(1)

    if queue is None:
        logging.error('Unable to get ' + str(queue) + ' queue by name')
        sys.exit(1)

    return queue

def main (yaml_facts=None, running_threads_last=0):

    if CFG['general']['yaml_facts'] == True:
        thread_get_yaml_facts = Timer(0, get_yaml_facts)
        thread_get_yaml_facts.setDaemon(True)
        thread_get_yaml_facts.start()

        logging.info('main: Waiting for initial facts')
        while yaml_facts is None:
            if yaml_queue.qsize() > 0:
                try:
                    yaml_facts = yaml_queue.get(False)
                except Empty:
                    logging.debug('main: Yaml queue is empty')
            time.sleep(1)
        logging.info('main: populated initial facts')

    thread_receiver = Timer(CFG['general']['sqs_poll_interval'], receive_sqs_msg, (yaml_facts,))
    thread_receiver.setDaemon(True)
    thread_receiver.start()

    thread_done_queue = Timer(CFG['general']['queue_poll_interval'], check_done_queue)
    thread_done_queue.setDaemon(True)
    thread_done_queue.start()

    thread_task_queue = Timer(CFG['general']['queue_poll_interval'], check_task_queue)
    thread_task_queue.setDaemon(True)
    thread_task_queue.start()

    try:
        while True:
            running_threads = active_count()
            if running_threads != running_threads_last:
                running_threads_last = running_threads
                logging.debug('main: ' + str(running_threads) + ' active threads')

            if not thread_receiver.is_alive():
                try:
                    thread_receiver.join(0.1)
                    thread_receiver = Timer(CFG['general']['sqs_poll_interval'], receive_sqs_msg, (yaml_facts,))
                    thread_receiver.setDaemon(True)
                    thread_receiver.start()
                except RuntimeError:
                    logging.error('main: RuntimeError in thread_receiver control')
                except:
                    logging.error('main: Uknown error in thread_receiver control')

            if not thread_done_queue.is_alive():
                try:
                    thread_done_queue.join(0.1)
                    thread_done_queue = Timer(CFG['general']['queue_poll_interval'], check_done_queue)
                    thread_done_queue.setDaemon(True)
                    thread_done_queue.start()
                except RuntimeError:
                    logging.error('main: RuntimeError in thread_done_queue control')
                except:
                    logging.error('main: Uknown error in thread_done_queue control')

            if not thread_task_queue.is_alive():
                try:
                    thread_task_queue.join(0.1)
                    thread_task_queue = Timer(CFG['general']['queue_poll_interval'], check_task_queue)
                    thread_task_queue.setDaemon(True)
                    thread_task_queue.start()
                except RuntimeError:
                    logging.error('main: RuntimeError in thread_task_queue control')
                except:
                    logging.error('main: Uknown error in thread_task_queue control')

            if CFG['general']['yaml_facts'] == True and not thread_get_yaml_facts.is_alive():
                try:
                    thread_get_yaml_facts.join(0.1)
                    thread_get_yaml_facts = Timer(CFG['general']['yaml_facts_refresh'], get_yaml_facts)
                    thread_get_yaml_facts.setDaemon(True)
                    thread_get_yaml_facts.start()
                except RuntimeError:
                    logging.error('main: RuntimeError in thread_get_yaml_facts control')
                except:
                    logging.error('main: Uknown error in thread_get_yaml_facts control')

                if yaml_queue.qsize() > 0:
                    try:
                        yaml_facts = yaml_queue.get(False)
                    except Empty:
                        logging.debug('main: Yaml queue is empty')

            # Be nice on the cpu
            time.sleep(CFG['general']['queue_poll_interval'])

    except (KeyboardInterrupt, SystemExit):
        logging.info('Main loop caught interrupt...')
        sys.exit(0)

if __name__ == "__main__":

    (foreground, logfile, pidfile, stdoutlog, stderrlog)=initialize()
    lockfile=daemonize(foreground, pidfile,'/dev/null', stdoutlog, stderrlog)
    try:
        sys.exit(main())
    except (KeyboardInterrupt, SystemExit):
        logging.info('ec2-cagent dutifully exiting...')
        sys.exit(0)
